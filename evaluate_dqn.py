# evaluate_dqn.py
#
# í›ˆë ¨ëœ DQN ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
# 'train_dqn.py'ì˜ QNetwork, DQNAgent í´ë˜ìŠ¤ ì •ì˜ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.
# 'flip_seven_env.py'ì˜ í™˜ê²½ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
#
import torch
import torch.nn as nn
import torch.optim as optim  # DQNAgent í´ë˜ìŠ¤ ë¡œë“œì— í•„ìš”
import numpy as np
import collections
import random
from typing import Dict, Tuple, Any
import gymnasium as gym
import time

# 'flip_seven_env.py' íŒŒì¼ì—ì„œ í™˜ê²½ í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
try:
    from flip_seven_env import FlipSevenCoreEnv
except ImportError:
    print("="*50)
    print("ì˜¤ë¥˜: 'flip_seven_env.py' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("evaluate_dqn.pyì™€ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    print("="*50)
    exit()

# ============================================================================
# í‰ê°€ ì„¤ì •
# ============================================================================
NUM_EVAL_GAMES = 100  # í‰ê°€í•  ì´ ê²Œì„ íšŸìˆ˜
MODEL_PATH = "./runs/dqn_flip7_final.pth"  # ë¶ˆëŸ¬ì˜¬ ëª¨ë¸ ê²½ë¡œ
GAME_GOAL_SCORE = 200  # ê²Œì„ ì¢…ë£Œ ëª©í‘œ ì ìˆ˜

# Device configuration
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")


# ============================================================================
# Q-NETWORK ARCHITECTURE (train_dqn.pyì™€ ë™ì¼í•´ì•¼ í•¨)
# ============================================================================
class QNetwork(nn.Module):
    """
    FlipSevenCoreEnvì˜ Dict ê´€ì¸¡ ê³µê°„ì„ ì²˜ë¦¬í•˜ëŠ” Q-Network.
    train_dqn.pyì˜ ì •ì˜ì™€ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ ëª¨ë¸ ë¡œë“œê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        hand_numbers_dim: int = 13,
        hand_modifiers_dim: int = 6,
        deck_composition_dim: int = 19,
        score_dim: int = 1,
        hidden_dim: int = 128
    ):
        super(QNetwork, self).__init__()
        
        # ê° ê´€ì¸¡ ìš”ì†Œë³„ ë³„ë„ ì²˜ë¦¬ ë ˆì´ì–´
        self.hand_numbers_net = nn.Sequential(
            nn.Linear(hand_numbers_dim, 32),
            nn.ReLU()
        )
        
        self.hand_modifiers_net = nn.Sequential(
            nn.Linear(hand_modifiers_dim, 16),
            nn.ReLU()
        )
        
        self.deck_composition_net = nn.Sequential(
            nn.Linear(deck_composition_dim, 64),
            nn.ReLU()
        )
        
        self.score_net = nn.Sequential(
            nn.Linear(score_dim, 8),
            nn.ReLU()
        )
        
        concat_dim = 32 + 16 + 64 + 8  # = 120
        
        # ê³µìœ  MLP ë ˆì´ì–´
        self.shared_net = nn.Sequential(
            nn.Linear(concat_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 2)  # ì¶œë ¥: Q(s, Stay), Q(s, Hit)
        )
    
    def forward(self, obs_dict: Dict[str, torch.Tensor]) -> torch.Tensor:
        hand_numbers_feat = self.hand_numbers_net(obs_dict["current_hand_numbers"])
        hand_modifiers_feat = self.hand_modifiers_net(obs_dict["current_hand_modifiers"])
        deck_composition_feat = self.deck_composition_net(obs_dict["deck_composition"])
        score_feat = self.score_net(obs_dict["total_game_score"])
        
        combined_feat = torch.cat([
            hand_numbers_feat,
            hand_modifiers_feat,
            deck_composition_feat,
            score_feat
        ], dim=1)
        
        q_values = self.shared_net(combined_feat)
        return q_values


# ============================================================================
# DQN AGENT (train_dqn.pyì™€ ë™ì¼, 'learn' ê´€ë ¨ ì œì™¸)
# ============================================================================
class DQNAgent:
    """
    DQN ì—ì´ì „íŠ¸. ëª¨ë¸ ë¡œë“œ ë° í–‰ë™ ì„ íƒ ê¸°ëŠ¥ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        action_space_size: int = 2,
        device: torch.device = DEVICE
    ):
        self.action_space_size = action_space_size
        self.device = device
        
        # Q-networks ì´ˆê¸°í™”
        self.q_network = QNetwork().to(device)
        self.target_network = QNetwork().to(device) # ë¡œë“œì— í•„ìš”
        self.optimizer = optim.Adam(self.q_network.parameters()) # ë¡œë“œì— í•„ìš”
        self.epsilon = 0.0 # í‰ê°€ ëª¨ë“œì´ë¯€ë¡œ 0ìœ¼ë¡œ ì„¤ì •
    
    def _dict_to_tensor(self, obs_dict: Dict[str, np.ndarray]) -> Dict[str, torch.Tensor]:
        """ê´€ì¸¡ ë”•ì…”ë„ˆë¦¬ë¥¼ í…ì„œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ë°°ì¹˜ í¬ê¸° 1)"""
        return {
            key: torch.FloatTensor(value).unsqueeze(0).to(self.device)
            for key, value in obs_dict.items()
        }
    
    def select_action(self, obs: Dict[str, np.ndarray], eval_mode: bool = False) -> int:
        """
        ì—¡ì‹¤ë¡ -ê·¸ë¦¬ë”” ì •ì±…ì— ë”°ë¼ í–‰ë™ ì„ íƒ.
        eval_mode=Trueì¼ ê²½ìš°, í•­ìƒ ê·¸ë¦¬ë””(Greedy) í–‰ë™ë§Œ ì„ íƒí•©ë‹ˆë‹¤.
        """
        # í‰ê°€ ëª¨ë“œì—ì„œëŠ” í•­ìƒ ìµœì ì˜ í–‰ë™(exploitation)ì„ ì„ íƒ
        if eval_mode or random.random() > self.epsilon:
            with torch.no_grad():
                obs_tensor = self._dict_to_tensor(obs)
                q_values = self.q_network(obs_tensor)
                action = q_values.argmax(dim=1).item()
            return action
        else:
            # (eval_mode=Falseì´ê³  ëœë¤ í™•ë¥ ì— ê±¸ë¦° ê²½ìš° - í‰ê°€ ì‹œì—ëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
            return random.randint(0, self.action_space_size - 1)
    
    def load(self, filepath: str):
        """ì €ì¥ëœ Q-network ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
        try:
            checkpoint = torch.load(filepath, map_location=self.device)
            self.q_network.load_state_dict(checkpoint['q_network_state_dict'])
            self.target_network.load_state_dict(checkpoint['target_network_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.epsilon = checkpoint.get('epsilon', 0.0) # í›ˆë ¨ ì¤‘ ì—¡ì‹¤ë¡  (í‰ê°€ ì‹œ ë¬´ê´€)
            self.q_network.eval() # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •! (í•„ìˆ˜)
            self.target_network.eval() # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •! (í•„ìˆ˜)
            print(f"ëª¨ë¸ì„ {filepath} ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        except FileNotFoundError:
            print(f"ì˜¤ë¥˜: {filepath} ì—ì„œ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            exit()
        except Exception as e:
            print(f"ì˜¤ë¥˜: ëª¨ë¸ ë¡œë“œ ì¤‘ ë¬¸ì œ ë°œìƒ. {e}")
            print("train_dqn.pyì˜ QNetwork êµ¬ì¡°ì™€ í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ êµ¬ì¡°ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            exit()


# ============================================================================
# EVALUATION FUNCTION
# ============================================================================
def run_evaluation(agent: DQNAgent, env: gym.Env, num_games: int):
    """
    í›ˆë ¨ëœ ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì„ 'num_games' ë§Œí¼ í‰ê°€í•©ë‹ˆë‹¤.
    """
    print("\n" + "=" * 70)
    print(f"ì—ì´ì „íŠ¸ í‰ê°€ ì‹œì‘ (ì´ {num_games} ê²Œì„)...")
    print("=" * 70)
    
    eval_rounds_per_game = []
    eval_scores_per_game = []
    total_start_time = time.time()
    
    for game in range(num_games):
        game_start_time = time.time()
        
        # --- 1. 'ê²Œì„' ì‹œì‘ ì‹œ ì „ì²´ ìƒíƒœ ìˆ˜ë™ ì´ˆê¸°í™” ---
        # (train_dqn.pyì™€ ë™ì¼í•œ ê²Œì„ ì´ˆê¸°í™” ë¡œì§)
        env.total_score = 0
        env.draw_deck = collections.deque()
        env.discard_pile = []
        env._initialize_deck_to_discard()  # 85ì¥ ë± ìƒì„±
        
        # ì²« ë¼ìš´ë“œë¥¼ ìœ„í•´ env.reset() í˜¸ì¶œ
        obs, info = env.reset(seed=42 + game)
        
        game_total_rounds = 0
        
        # --- 2. 'ê²Œì„' ë£¨í”„ (ëª©í‘œ ì ìˆ˜ ë„ë‹¬ê¹Œì§€) ---
        while info.get("total_game_score", 0) < GAME_GOAL_SCORE:
            game_total_rounds += 1
            terminated = False  # 'ë¼ìš´ë“œ' ì¢…ë£Œ í”Œë˜ê·¸
            
            # --- 3. 'ë¼ìš´ë“œ' ë£¨í”„ (Bust, Stay, Flip 7 ì „ê¹Œì§€) ---
            while not terminated:
                
                # í–‰ë™ ì„ íƒ (eval_mode=True: Epsilon-greedyê°€ ì•„ë‹Œ Greedy ì„ íƒ)
                action = agent.select_action(obs, eval_mode=True)
                
                # í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©
                next_obs, reward, terminated, truncated, info = env.step(action)
                
                # ë‹¤ìŒ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                obs = next_obs
            
            # --- 4. ë¼ìš´ë“œ ì¢…ë£Œ ---
            # ê²Œì„ì´ ëë‚˜ì§€ ì•Šì•˜ë‹¤ë©´, ë‹¤ìŒ ë¼ìš´ë“œë¥¼ ìœ„í•´ reset() í˜¸ì¶œ (ì†íŒ¨ë§Œ ë¹„ì›€)
            if info.get("total_game_score", 0) < GAME_GOAL_SCORE:
                obs, info = env.reset()

        # --- 5. ê²Œì„ ì¢…ë£Œ ---
        game_end_time = time.time()
        final_score = info.get("total_game_score", 0)
        eval_rounds_per_game.append(game_total_rounds)
        eval_scores_per_game.append(final_score)
        
        print(f"  [ê²Œì„ {game + 1:03d}/{num_games}] "
              f"ìµœì¢… ì ìˆ˜: {final_score:03d} | "
              f"ì´ ë¼ìš´ë“œ: {game_total_rounds:02d} | "
              f"ì†Œìš” ì‹œê°„: {game_end_time - game_start_time:.2f}ì´ˆ")

    # --- 6. ìµœì¢… ê²°ê³¼ ìš”ì•½ ---
    total_end_time = time.time()
    print("=" * 70)
    print(f"ğŸ í‰ê°€ ì™„ë£Œ (ì´ {num_games} ê²Œì„) ğŸ")
    print(f"  - ì´ ì†Œìš” ì‹œê°„: {total_end_time - total_start_time:.2f}ì´ˆ")
    print(f"  - í‰ê·  ë¼ìš´ë“œ ìˆ˜: {np.mean(eval_rounds_per_game):.2f} ë¼ìš´ë“œ")
    print(f"  - ìµœì†Œ ë¼ìš´ë“œ ìˆ˜: {np.min(eval_rounds_per_game)}")
    print(f"  - ìµœëŒ€ ë¼ìš´ë“œ ìˆ˜: {np.max(eval_rounds_per_game)}")
    print(f"  - í‰ê·  ìµœì¢… ì ìˆ˜: {np.mean(eval_scores_per_game):.2f} ì ")
    print("=" * 70)


# ============================================================================
# ENTRY POINT
# ============================================================================
if __name__ == "__main__":
    
    # 1. í™˜ê²½ ìƒì„±
    try:
        env = FlipSevenCoreEnv()
        print("[ì„±ê³µ] FlipSevenCoreEnv í™˜ê²½ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"[ì‹¤íŒ¨] í™˜ê²½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        exit()

    # 2. ì—ì´ì „íŠ¸ ìƒì„± ë° ëª¨ë¸ ë¡œë“œ
    agent = DQNAgent(device=DEVICE)
    agent.load(MODEL_PATH)
    
    # 3. í‰ê°€ ì‹¤í–‰
    run_evaluation(agent, env, num_games=NUM_EVAL_GAMES)