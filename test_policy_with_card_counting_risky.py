"""
íŒŒì¼: test_policy_with_card_counting_risky.py
ëª©ì : 'ì¹´ë“œ ì¹´ìš´íŒ…' í•™ìŠµ ì—¬ë¶€ë¥¼ ê²€ì¦í•˜ê¸° ìœ„í•œ ê³ ìœ„í—˜(high-risk) ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸.
"""

import os
# OpenMP ì¤‘ë³µ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶©ëŒ ë°©ì§€
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import collections
from typing import Dict, Any
import gymnasium as gym
import matplotlib.pyplot as plt  # ì‹œê°í™”ìš©

from flip_seven_env import FlipSevenCoreEnv, CARD_TO_IDX, MODIFIER_TO_IDX, NUMBER_CARD_TYPES, MODIFIER_CARD_TYPES

# Device configuration
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# ============================================================================
# Q-NETWORK ARCHITECTURE (train_dqn.pyì™€ ë™ì¼)
# ============================================================================
class QNetwork(nn.Module):
    """
    Q-Network that processes the Dict observation space from FlipSevenCoreEnv.
    """
    
    def __init__(
        self,
        hand_numbers_dim: int = 13,
        hand_modifiers_dim: int = 6,
        deck_composition_dim: int = 19,
        score_dim: int = 1,
        hidden_dim: int = 128
    ):
        super(QNetwork, self).__init__()
        
        # Separate processing layers for each observation component
        self.hand_numbers_net = nn.Sequential(
            nn.Linear(hand_numbers_dim, 32),
            nn.ReLU()
        )
        
        self.hand_modifiers_net = nn.Sequential(
            nn.Linear(hand_modifiers_dim, 16),
            nn.ReLU()
        )
        
        self.deck_composition_net = nn.Sequential(
            nn.Linear(deck_composition_dim, 64),
            nn.ReLU()
        )
        
        self.score_net = nn.Sequential(
            nn.Linear(score_dim, 8),
            nn.ReLU()
        )
        
        concat_dim = 32 + 16 + 64 + 8  # = 120
        
        # Shared MLP layers
        self.shared_net = nn.Sequential(
            nn.Linear(concat_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 2)  # Output: Q(s, Stay), Q(s, Hit)
        )
    
    def forward(self, obs_dict: Dict[str, torch.Tensor]) -> torch.Tensor:
        hand_numbers_feat = self.hand_numbers_net(obs_dict["current_hand_numbers"])
        hand_modifiers_feat = self.hand_modifiers_net(obs_dict["current_hand_modifiers"])
        deck_composition_feat = self.deck_composition_net(obs_dict["deck_composition"])
        score_feat = self.score_net(obs_dict["total_game_score"])
        
        combined_feat = torch.cat([
            hand_numbers_feat,
            hand_modifiers_feat,
            deck_composition_feat,
            score_feat
        ], dim=1)
        
        q_values = self.shared_net(combined_feat)
        return q_values


# ============================================================================
# DQN AGENT (test_policy_scenarios.pyì™€ ë™ì¼)
# ============================================================================
class DQNAgent:
    """
    DQN ì—ì´ì „íŠ¸ (ëª¨ë¸ ë¡œë“œ ë° Q-values ì¡°íšŒë§Œ ì‚¬ìš©)
    """
    
    def __init__(self, device: torch.device = DEVICE):
        self.device = device
        self.q_network = QNetwork().to(device)
        self.target_network = QNetwork().to(device)
        self.optimizer = optim.Adam(self.q_network.parameters())
        self.epsilon = 0.0
    
    def load(self, filepath: str):
        """ì €ì¥ëœ Q-network ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])
        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.q_network.eval()
        self.target_network.eval()
        print(f"ëª¨ë¸ì„ {filepath} ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\n")
    
    def _dict_to_tensor(self, obs_dict: Dict[str, np.ndarray]) -> Dict[str, torch.Tensor]:
        """ê´€ì°° ë”•ì…”ë„ˆë¦¬ë¥¼ í…ì„œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        return {
            key: torch.FloatTensor(value).unsqueeze(0).to(self.device)
            for key, value in obs_dict.items()
        }


# ============================================================================
# HELPER FUNCTIONS (test_policy_scenarios.pyì™€ ë™ì¼)
# ============================================================================
def create_obs(hand_nums: set, hand_mods: list, deck_list: list, total_score: int) -> Dict[str, np.ndarray]:
    """
    ìˆ˜ë™ìœ¼ë¡œ ê´€ì¸¡ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        hand_nums: ì†ì— ìˆëŠ” ìˆ«ì ì¹´ë“œ ì§‘í•© (ì˜ˆ: {8, 12})
        hand_mods: ì†ì— ìˆëŠ” ìˆ˜ì •ì ì¹´ë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['+4', 'x2'])
        deck_list: ë±ì— ë‚¨ì•„ìˆëŠ” ì¹´ë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['12', '11', ..., '+2'])
        total_score: í˜„ì¬ ê²Œì„ ì´ì 
    
    Returns:
        observation dictionary
    """
    # 1. current_hand_numbers
    hand_num_obs = np.zeros(13, dtype=np.int32)
    for num in hand_nums:
        hand_num_obs[num] = 1
    
    # 2. current_hand_modifiers
    hand_mod_obs = np.zeros(6, dtype=np.int32)
    for mod in hand_mods:
        hand_mod_obs[MODIFIER_TO_IDX[mod]] = 1
    
    # 3. deck_composition
    deck_comp_obs = np.zeros(19, dtype=np.int32)
    for card in deck_list:
        deck_comp_obs[CARD_TO_IDX[card]] += 1
    
    # 4. total_game_score
    total_score_obs = np.array([total_score], dtype=np.int32)
    
    return {
        "current_hand_numbers": hand_num_obs,
        "current_hand_modifiers": hand_mod_obs,
        "deck_composition": deck_comp_obs,
        "total_game_score": total_score_obs
    }


def get_q_values(agent: DQNAgent, env_state: Dict[str, np.ndarray]):
    """
    ì—ì´ì „íŠ¸ì˜ Q-valuesë¥¼ ê³„ì‚°í•˜ê³  ì¶œë ¥í•©ë‹ˆë‹¤.
    
    Args:
        agent: DQN ì—ì´ì „íŠ¸
        env_state: ê´€ì¸¡ ë”•ì…”ë„ˆë¦¬
    """
    # Convert to tensor (batch size = 1)
    obs_tensor = {
        key: torch.FloatTensor(value).unsqueeze(0).to(agent.device)
        for key, value in env_state.items()
    }
    
    # Get Q-values
    with torch.no_grad():
        q_values = agent.q_network(obs_tensor)
        q_stay = q_values[0, 0].item()
        q_hit = q_values[0, 1].item()
    
    # Print results
    print(f"    Q(Stay): {q_stay:7.2f} | Q(Hit): {q_hit:7.2f}")
    
    # Determine action
    if q_stay > q_hit:
        print(f"    â†’ ì„ íƒ: Stay (Q-value ì°¨ì´: {q_stay - q_hit:.2f})")
    else:
        print(f"    â†’ ì„ íƒ: Hit (Q-value ì°¨ì´: {q_hit - q_stay:.2f})")
    print()


def create_full_deck() -> list:
    """
    ì „ì²´ 85ì¥ ì¹´ë“œ ë±ì„ ìƒì„±í•©ë‹ˆë‹¤ (í™˜ê²½ì˜ _initialize_deck_to_discardì™€ ë™ì¼).
    """
    deck = []
    # Number Cards (79 total)
    for i in range(1, 13):
        deck.extend([str(i)] * i)
    deck.append("0")  # 1x "0" card
    
    # Modifier Cards (6 total)
    deck.extend(MODIFIER_CARD_TYPES)
    
    return deck


# ============================================================================
# MAIN EXECUTION
# ============================================================================
if __name__ == "__main__":
    
    # 6-1. ëª¨ë¸ê³¼ ë±ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    print("=" * 70)
    print("ğŸ“Š Scenario: High-Risk Card Counting (ê³ ìœ„í—˜ ì¹´ë“œ ì¹´ìš´íŒ…)")
    print("=" * 70)
    print("ì„¤ëª…: Bust ìœ„í—˜ì´ ë†’ì€ ì†íŒ¨({12, 11, 10, 7})ë¥¼ ê¸°ì¤€ìœ¼ë¡œ,")
    print("      ë± ìƒíƒœ(ìœ„í—˜/ì•ˆì „)ì— ë”°ë¼ ì—ì´ì „íŠ¸ì˜ *í–‰ë™*ì´ ë°”ë€ŒëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.")
    print("=" * 70)
    print()
    
    agent = DQNAgent(device=DEVICE)
    agent.load('./runs/dqn_flip7_final.pth')
    full_deck = create_full_deck()
    
    # 6-2. í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ìƒíƒœë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
    # (ì´ì „ í…ŒìŠ¤íŠ¸ì˜ í•œê³„: ì†íŒ¨ê°€ 1ì¥ì´ë¼ Bust ìœ„í—˜ì´ ë„ˆë¬´ ë‚®ì•˜ìŒ)
    # (ê°œì„ : ì†íŒ¨ 4ì¥, 40ì ì˜ ê³ ê°€ì¹˜/ê³ ìœ„í—˜ ìƒíƒœë¡œ ì„¤ì •)
    risky_hand_set = {12, 11, 10, 7}  # 40ì 
    risky_hand_str_set = {'12', '11', '10', '7'}
    neutral_total_score = 50

    q_values_risk = {}
    q_values_safe = {}

    # 6-3. Case A (Bust ìœ„í—˜ ë†’ìŒ) Q-value ê³„ì‚°
    print("\n[Case A] ë±ì— Bust ìœ ë°œ ì¹´ë“œê°€ ë‚¨ì•„ìˆìŒ (Bust ìœ„í—˜ ë†’ìŒ)")
    print(f"  ì†íŒ¨: {sorted(list(risky_hand_set))} (40ì )")
    print(f"  ë±: ì „ì²´ 85ì¥ (ì†íŒ¨ ì¹´ë“œ {len(risky_hand_str_set)}ì¢… ëª¨ë‘ í¬í•¨)")
    deck_risk = full_deck  # ë±ì— {12, 11, 10, 7}ì´ ëª¨ë‘ ë‚¨ì•„ìˆìŒ
    obs_risk = create_obs(risky_hand_set, [], deck_risk, neutral_total_score)
    
    # get_q_valuesëŠ” ì¶œë ¥ì„ í¬í•¨í•˜ë¯€ë¡œ, Q-valueë§Œ ë”°ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    with torch.no_grad():
        obs_tensor = agent._dict_to_tensor(obs_risk)
        q_vals = agent.q_network(obs_tensor)
        q_values_risk['Stay'] = q_vals[0, 0].item()
        q_values_risk['Hit'] = q_vals[0, 1].item()
    
    get_q_values(agent, obs_risk)  # ì½˜ì†” ì¶œë ¥ìš©

    # 6-4. Case B (Bust ìœ„í—˜ ì—†ìŒ) Q-value ê³„ì‚°
    print("[Case B] ë±ì— Bust ìœ ë°œ ì¹´ë“œê°€ ì—†ìŒ (Bust ë¶ˆê°€ëŠ¥)")
    print(f"  ì†íŒ¨: {sorted(list(risky_hand_set))} (40ì )")
    print(f"  ë±: ì†íŒ¨ ì¹´ë“œ {len(risky_hand_str_set)}ì¢… ì œì™¸ (Bust ë¶ˆê°€ëŠ¥)")
    deck_safe = [card for card in full_deck if card not in risky_hand_str_set]
    obs_safe = create_obs(risky_hand_set, [], deck_safe, neutral_total_score)
    
    with torch.no_grad():
        obs_tensor = agent._dict_to_tensor(obs_safe)
        q_vals = agent.q_network(obs_tensor)
        q_values_safe['Stay'] = q_vals[0, 0].item()
        q_values_safe['Hit'] = q_vals[0, 1].item()
        
    get_q_values(agent, obs_safe)  # ì½˜ì†” ì¶œë ¥ìš©

    # 6-5. ìµœì¢… ê²°ë¡  ì¶œë ¥
    print("=" * 70)
    print("ğŸ“ˆ ìµœì¢… ë¶„ì„ ê²°ê³¼ ğŸ“ˆ")
    print("=" * 70)
    action_risk = "Stay" if q_values_risk['Stay'] > q_values_risk['Hit'] else "Hit"
    action_safe = "Stay" if q_values_safe['Stay'] > q_values_safe['Hit'] else "Hit"

    print(f"  - Case A (ìœ„í—˜): Q(Stay)={q_values_risk['Stay']:.2f} vs Q(Hit)={q_values_risk['Hit']:.2f}  ->  ì„ íƒ: {action_risk}")
    print(f"  - Case B (ì•ˆì „): Q(Stay)={q_values_safe['Stay']:.2f} vs Q(Hit)={q_values_safe['Hit']:.2f}  ->  ì„ íƒ: {action_safe}")
    print()
    
    # Q(Hit) ì°¨ì´ ë¶„ì„
    q_hit_diff = q_values_safe['Hit'] - q_values_risk['Hit']
    print(f"  - Q(Hit) ì°¨ì´ (Safe - Risk): {q_hit_diff:+.2f}")
    
    if q_hit_diff > 0:
        print(f"    âœ“ ë±ì´ ì•ˆì „í•  ë•Œ Hitì˜ Q-valueê°€ {q_hit_diff:.2f}ë§Œí¼ ë” ë†’ìŠµë‹ˆë‹¤.")
        print(f"    âœ“ ì—ì´ì „íŠ¸ê°€ ì¹´ë“œ ì¹´ìš´íŒ…ì„ í†µí•´ ìœ„í—˜ë„ë¥¼ í‰ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
    else:
        print(f"    âœ— ë± ìƒíƒœê°€ Q(Hit)ì— ê¸ì •ì  ì˜í–¥ì„ ì£¼ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    print()

    if action_risk == "Stay" and action_safe == "Hit":
        print("  [ê²°ë¡ ] âœ… ì„±ê³µ: ì—ì´ì „íŠ¸ê°€ ì¹´ë“œ ì¹´ìš´íŒ…ì„ ê¸°ë°˜ìœ¼ë¡œ ì •ì±…ì„ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.")
        print("         ìœ„í—˜í•œ ìƒí™©ì—ì„œëŠ” Stayë¥¼, ì•ˆì „í•œ ìƒí™©ì—ì„œëŠ” Hitë¥¼ ì„ íƒí•©ë‹ˆë‹¤.")
    elif action_risk == action_safe:
        print(f"  [ê²°ë¡ ] âš ï¸  ë¶€ë¶„ ì„±ê³µ: ì—ì´ì „íŠ¸ê°€ ë‘ ìƒí™© ëª¨ë‘ '{action_risk}'ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤.")
        print("         ë± ìƒíƒœì— ë”°ë¥¸ ëª…í™•í•œ ì •ì±… ë³€í™”ëŠ” ê´€ì°°ë˜ì§€ ì•Šì•˜ì§€ë§Œ,")
        print("         Q-value ì°¨ì´ë¥¼ í†µí•´ ìœ„í—˜ë„ ì¸ì‹ì€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("  [ê²°ë¡ ] âŒ ì‹¤íŒ¨: ì—ì´ì „íŠ¸ê°€ ë± ìƒíƒœì— ë”°ë¼ ì˜ˆìƒê³¼ ë‹¤ë¥¸ í–‰ë™ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤.")
    print("=" * 70)

    # 7. ì‹œê°í™” ë¡œì§
    print("\nì‹œê°í™” ìƒì„± ì¤‘...")
    
    labels = ['Case A (Risk)', 'Case B (Safe)']
    q_stay_values = [q_values_risk['Stay'], q_values_safe['Stay']]
    q_hit_values = [q_values_risk['Hit'], q_values_safe['Hit']]

    x = np.arange(len(labels))
    width = 0.35

    fig, ax = plt.subplots(figsize=(10, 6))
    rects1 = ax.bar(x - width/2, q_stay_values, width, label='Q(Stay)', 
                    color='salmon', edgecolor='black', linewidth=1.2)
    rects2 = ax.bar(x + width/2, q_hit_values, width, label='Q(Hit)', 
                    color='mediumturquoise', edgecolor='black', linewidth=1.2)

    ax.set_ylabel('Q-Value', fontsize=12, fontweight='bold')
    ax.set_title('High-Risk Card Counting Analysis\n(Hand: {12, 11, 10, 7} = 40 points)', 
                 fontsize=14, fontweight='bold', pad=15)
    ax.set_xticks(x)
    ax.set_xticklabels(labels, fontsize=11)
    ax.legend(fontsize=11)
    ax.grid(axis='y', linestyle='--', alpha=0.5)
    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8, alpha=0.3)

    # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
    ax.bar_label(rects1, padding=3, fmt='%.2f', fontweight='bold')
    ax.bar_label(rects2, padding=3, fmt='%.2f', fontweight='bold')

    fig.tight_layout()
    
    save_path = './runs/policy_analysis_high_risk_counting.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {save_path}")
    print("=" * 70)
