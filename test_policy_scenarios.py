# test_policy_scenarios.py
#
# ìµœì¢… í›ˆë ¨ëœ DQN ì—ì´ì „íŠ¸ì˜ Q-valuesë¥¼ íŠ¹ì • ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë¶„ì„í•©ë‹ˆë‹¤.
# ì´ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ê°€ ì¹´ë“œ ì¹´ìš´íŒ…ê³¼ ëª©í‘œ ì¸ì‹ì„ í•™ìŠµí–ˆëŠ”ì§€ ì •ì„±ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
#
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import collections
from typing import Dict, Any
import gymnasium as gym

from flip_seven_env import FlipSevenCoreEnv, CARD_TO_IDX, MODIFIER_TO_IDX, NUMBER_CARD_TYPES, MODIFIER_CARD_TYPES

# Device configuration
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# ============================================================================
# Q-NETWORK ARCHITECTURE (train_dqn.pyì™€ ë™ì¼)
# ============================================================================
class QNetwork(nn.Module):
    """
    Q-Network that processes the Dict observation space from FlipSevenCoreEnv.
    """
    
    def __init__(
        self,
        hand_numbers_dim: int = 13,
        hand_modifiers_dim: int = 6,
        deck_composition_dim: int = 19,
        score_dim: int = 1,
        hidden_dim: int = 128
    ):
        super(QNetwork, self).__init__()
        
        # Separate processing layers for each observation component
        self.hand_numbers_net = nn.Sequential(
            nn.Linear(hand_numbers_dim, 32),
            nn.ReLU()
        )
        
        self.hand_modifiers_net = nn.Sequential(
            nn.Linear(hand_modifiers_dim, 16),
            nn.ReLU()
        )
        
        self.deck_composition_net = nn.Sequential(
            nn.Linear(deck_composition_dim, 64),
            nn.ReLU()
        )
        
        self.score_net = nn.Sequential(
            nn.Linear(score_dim, 8),
            nn.ReLU()
        )
        
        concat_dim = 32 + 16 + 64 + 8  # = 120
        
        # Shared MLP layers
        self.shared_net = nn.Sequential(
            nn.Linear(concat_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 2)  # Output: Q(s, Stay), Q(s, Hit)
        )
    
    def forward(self, obs_dict: Dict[str, torch.Tensor]) -> torch.Tensor:
        hand_numbers_feat = self.hand_numbers_net(obs_dict["current_hand_numbers"])
        hand_modifiers_feat = self.hand_modifiers_net(obs_dict["current_hand_modifiers"])
        deck_composition_feat = self.deck_composition_net(obs_dict["deck_composition"])
        score_feat = self.score_net(obs_dict["total_game_score"])
        
        combined_feat = torch.cat([
            hand_numbers_feat,
            hand_modifiers_feat,
            deck_composition_feat,
            score_feat
        ], dim=1)
        
        q_values = self.shared_net(combined_feat)
        return q_values


# ============================================================================
# DQN AGENT (ê°„ì†Œí™” ë²„ì „)
# ============================================================================
class DQNAgent:
    """
    DQN ì—ì´ì „íŠ¸ (ëª¨ë¸ ë¡œë“œ ë° Q-values ì¡°íšŒë§Œ ì‚¬ìš©)
    """
    
    def __init__(self, device: torch.device = DEVICE):
        self.device = device
        self.q_network = QNetwork().to(device)
        self.target_network = QNetwork().to(device)
        self.optimizer = optim.Adam(self.q_network.parameters())
        self.epsilon = 0.0
    
    def load(self, filepath: str):
        """ì €ì¥ëœ Q-network ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])
        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.q_network.eval()
        self.target_network.eval()
        print(f"ëª¨ë¸ì„ {filepath} ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\n")


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================
def create_obs(hand_nums: set, hand_mods: list, deck_list: list, total_score: int) -> Dict[str, np.ndarray]:
    """
    ìˆ˜ë™ìœ¼ë¡œ ê´€ì¸¡ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        hand_nums: ì†ì— ìˆëŠ” ìˆ«ì ì¹´ë“œ ì§‘í•© (ì˜ˆ: {8, 12})
        hand_mods: ì†ì— ìˆëŠ” ìˆ˜ì •ì ì¹´ë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['+4', 'x2'])
        deck_list: ë±ì— ë‚¨ì•„ìˆëŠ” ì¹´ë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['12', '11', ..., '+2'])
        total_score: í˜„ì¬ ê²Œì„ ì´ì 
    
    Returns:
        observation dictionary
    """
    # 1. current_hand_numbers
    hand_num_obs = np.zeros(13, dtype=np.int32)
    for num in hand_nums:
        hand_num_obs[num] = 1
    
    # 2. current_hand_modifiers
    hand_mod_obs = np.zeros(6, dtype=np.int32)
    for mod in hand_mods:
        hand_mod_obs[MODIFIER_TO_IDX[mod]] = 1
    
    # 3. deck_composition
    deck_comp_obs = np.zeros(19, dtype=np.int32)
    for card in deck_list:
        deck_comp_obs[CARD_TO_IDX[card]] += 1
    
    # 4. total_game_score
    total_score_obs = np.array([total_score], dtype=np.int32)
    
    return {
        "current_hand_numbers": hand_num_obs,
        "current_hand_modifiers": hand_mod_obs,
        "deck_composition": deck_comp_obs,
        "total_game_score": total_score_obs
    }


def get_q_values(agent: DQNAgent, env_state: Dict[str, np.ndarray]):
    """
    ì—ì´ì „íŠ¸ì˜ Q-valuesë¥¼ ê³„ì‚°í•˜ê³  ì¶œë ¥í•©ë‹ˆë‹¤.
    
    Args:
        agent: DQN ì—ì´ì „íŠ¸
        env_state: ê´€ì¸¡ ë”•ì…”ë„ˆë¦¬
    """
    # Convert to tensor (batch size = 1)
    obs_tensor = {
        key: torch.FloatTensor(value).unsqueeze(0).to(agent.device)
        for key, value in env_state.items()
    }
    
    # Get Q-values
    with torch.no_grad():
        q_values = agent.q_network(obs_tensor)
        q_stay = q_values[0, 0].item()
        q_hit = q_values[0, 1].item()
    
    # Print results
    print(f"    Q(Stay): {q_stay:7.2f} | Q(Hit): {q_hit:7.2f}")
    
    # Determine action
    if q_stay > q_hit:
        print(f"    â†’ ì„ íƒ: Stay (Q-value ì°¨ì´: {q_stay - q_hit:.2f})")
    else:
        print(f"    â†’ ì„ íƒ: Hit (Q-value ì°¨ì´: {q_hit - q_stay:.2f})")
    print()


def create_full_deck() -> list:
    """
    ì „ì²´ 85ì¥ ì¹´ë“œ ë±ì„ ìƒì„±í•©ë‹ˆë‹¤ (í™˜ê²½ì˜ _initialize_deck_to_discardì™€ ë™ì¼).
    """
    deck = []
    # Number Cards (79 total)
    for i in range(1, 13):
        deck.extend([str(i)] * i)
    deck.append("0")  # 1x "0" card
    
    # Modifier Cards (6 total)
    deck.extend(MODIFIER_CARD_TYPES)
    
    return deck


# ============================================================================
# MAIN EXECUTION
# ============================================================================
if __name__ == "__main__":
    
    print("=" * 70)
    print("Policy Scenario Testing: Analyzing Learned Q-Values")
    print("=" * 70)
    print()
    
    # 1. ì—ì´ì „íŠ¸ ë¡œë“œ
    agent = DQNAgent(device=DEVICE)
    agent.load('./runs/dqn_flip7_final.pth')
    
    # 2. ì „ì²´ ë± ìƒì„±
    full_deck = create_full_deck()
    print(f"ì „ì²´ ë± ìƒì„± ì™„ë£Œ: {len(full_deck)}ì¥\n")
    
    # ========================================================================
    # SCENARIO 1: Card Counting Test
    # ========================================================================
    print("=" * 70)
    print("ğŸ“Š Scenario 1: Card Counting (ì¹´ë“œ ì¹´ìš´íŒ… í•™ìŠµ ì—¬ë¶€)")
    print("=" * 70)
    print("ì„¤ëª…: ì†ì— '8'ì´ ìˆì„ ë•Œ, ë±ì— '8'ì´ ë‚¨ì•„ìˆëŠ”ì§€ ì—¬ë¶€ì— ë”°ë¼")
    print("      ì—ì´ì „íŠ¸ì˜ í–‰ë™ì´ ë‹¬ë¼ì§€ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.")
    print("=" * 70)
    print()
    
    s1_hand = {8}
    
    # Case A: ë±ì— '8'ì´ ì—¬ì „íˆ ë‚¨ì•„ìˆìŒ (ìœ„í—˜)
    print("  [Case A] ë±ì— '8'ì´ ë‚¨ì•„ìˆìŒ (Bust ìœ„í—˜ ìˆìŒ)")
    s1_deck_with_8 = [card for card in full_deck if card != '8']  # '8' í•˜ë‚˜ë§Œ ì œê±°
    s1_obs_safe = create_obs(s1_hand, [], s1_deck_with_8, 50)
    get_q_values(agent, s1_obs_safe)
    
    # Case B: ë±ì— '8'ì´ ì „í˜€ ì—†ìŒ (ì•ˆì „)
    print("  [Case B] ë±ì— '8'ì´ ì „í˜€ ì—†ìŒ (Bust ë¶ˆê°€ëŠ¥)")
    s1_deck_no_8 = [card for card in full_deck if card not in ['8']]  # ëª¨ë“  '8' ì œê±°
    s1_obs_bust_proof = create_obs(s1_hand, [], s1_deck_no_8, 50)
    get_q_values(agent, s1_obs_bust_proof)
    
    print("  âœ“ ì˜ˆìƒ ê²°ê³¼: Case Bì—ì„œ Hitì˜ Q-valueê°€ ë” ë†’ì•„ì•¼ í•¨")
    print("  âœ“ ì´ëŠ” ì—ì´ì „íŠ¸ê°€ ì¹´ë“œ ì¹´ìš´íŒ…ì„ í•™ìŠµí–ˆìŒì„ ì˜ë¯¸í•¨")
    print()
    
    # ========================================================================
    # SCENARIO 2: Goal Awareness Test
    # ========================================================================
    print("=" * 70)
    print("ğŸ¯ Scenario 2: Goal Awareness (ëª©í‘œ ì¸ì‹ í•™ìŠµ ì—¬ë¶€)")
    print("=" * 70)
    print("ì„¤ëª…: ë™ì¼í•œ ë¼ìš´ë“œ ì ìˆ˜(25ì )ë¥¼ ê°€ì§€ê³  ìˆì„ ë•Œ,")
    print("      ê²Œì„ ì´ì ì— ë”°ë¼ ì—ì´ì „íŠ¸ì˜ í–‰ë™ì´ ë‹¬ë¼ì§€ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.")
    print("=" * 70)
    print()
    
    s2_hand = {12, 7, 6}  # 12 + 7 + 6 = 25 points
    s2_deck = full_deck  # ê°„ë‹¨íˆ ì „ì²´ ë± ì‚¬ìš©
    
    # Case A: ì´ì  100 (Stayí•´ë„ 125ì ì´ë¯€ë¡œ 200ì ì— ëª» ë¯¸ì¹¨)
    print("  [Case A] í˜„ì¬ ì´ì : 100 (Stay ì‹œ 125ì  â†’ 200ì  ë¯¸ë‹¬)")
    s2_obs_far = create_obs(s2_hand, [], s2_deck, 100)
    get_q_values(agent, s2_obs_far)
    
    # Case B: ì´ì  180 (Stayí•˜ë©´ 205ì ì´ë¯€ë¡œ ê²Œì„ ìŠ¹ë¦¬!)
    print("  [Case B] í˜„ì¬ ì´ì : 180 (Stay ì‹œ 205ì  â†’ ê²Œì„ ìŠ¹ë¦¬!)")
    s2_obs_close = create_obs(s2_hand, [], s2_deck, 180)
    get_q_values(agent, s2_obs_close)
    
    print("  âœ“ ì˜ˆìƒ ê²°ê³¼: Case Bì—ì„œ Stayì˜ Q-valueê°€ í›¨ì”¬ ë†’ì•„ì•¼ í•¨")
    print("  âœ“ ì´ëŠ” ì—ì´ì „íŠ¸ê°€ 200ì  ëª©í‘œë¥¼ ì¸ì‹í•˜ê³  ìˆìŒì„ ì˜ë¯¸í•¨")
    print()
    
    # ========================================================================
    # SCENARIO 3: Risk vs. Reward (ì¶”ê°€ ì‹œë‚˜ë¦¬ì˜¤)
    # ========================================================================
    print("=" * 70)
    print("âš–ï¸  Scenario 3: Risk vs. Reward (ìœ„í—˜ ëŒ€ë¹„ ë³´ìƒ í‰ê°€)")
    print("=" * 70)
    print("ì„¤ëª…: ë‚®ì€ ì ìˆ˜ë¥¼ ê°€ì§€ê³  ìˆì„ ë•Œì™€ ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§€ê³  ìˆì„ ë•Œ")
    print("      ì—ì´ì „íŠ¸ì˜ ìœ„í—˜ ê°ìˆ˜ ì„±í–¥ì´ ë‹¬ë¼ì§€ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.")
    print("=" * 70)
    print()
    
    # Case A: ë‚®ì€ ì ìˆ˜ (5ì ) - Hit í•´ì•¼ í•¨
    print("  [Case A] ì†íŒ¨: {5}, ë¼ìš´ë“œ ì ìˆ˜: 5ì  (ë„ˆë¬´ ë‚®ìŒ)")
    s3_hand_low = {5}
    s3_obs_low = create_obs(s3_hand_low, [], full_deck, 50)
    get_q_values(agent, s3_obs_low)
    
    # Case B: ë†’ì€ ì ìˆ˜ (40ì  ì´ìƒ) - Stay ê³ ë ¤
    print("  [Case B] ì†íŒ¨: {12, 11, 10, 7}, ë¼ìš´ë“œ ì ìˆ˜: 40ì  (ë†’ìŒ)")
    s3_hand_high = {12, 11, 10, 7}
    s3_obs_high = create_obs(s3_hand_high, [], full_deck, 50)
    get_q_values(agent, s3_obs_high)
    
    print("  âœ“ ì˜ˆìƒ ê²°ê³¼: Case Aì—ì„œëŠ” Hit, Case Bì—ì„œëŠ” Stayê°€ ì„ í˜¸ë˜ì–´ì•¼ í•¨")
    print("  âœ“ ì´ëŠ” ì—ì´ì „íŠ¸ê°€ ì ìˆ˜ ê¸°ë°˜ ìœ„í—˜ ê´€ë¦¬ë¥¼ í•™ìŠµí–ˆìŒì„ ì˜ë¯¸í•¨")
    print()
    
    # ========================================================================
    # SCENARIO 4: Modifier Card Effect
    # ========================================================================
    print("=" * 70)
    print("âœ¨ Scenario 4: Modifier Card Effect (ìˆ˜ì •ì ì¹´ë“œ ì˜í–¥)")
    print("=" * 70)
    print("ì„¤ëª…: ìˆ˜ì •ì ì¹´ë“œ(x2)ê°€ ìˆì„ ë•Œ ì—ì´ì „íŠ¸ì˜ í‰ê°€ê°€ ë‹¬ë¼ì§€ëŠ”ì§€ í™•ì¸")
    print("=" * 70)
    print()
    
    # Case A: ìˆ˜ì •ì ì—†ìŒ
    print("  [Case A] ì†íŒ¨: {10, 5}, ìˆ˜ì •ì: ì—†ìŒ (15ì )")
    s4_hand = {10, 5}
    s4_obs_no_mod = create_obs(s4_hand, [], full_deck, 50)
    get_q_values(agent, s4_obs_no_mod)
    
    # Case B: x2 ìˆ˜ì •ì ìˆìŒ
    print("  [Case B] ì†íŒ¨: {10, 5}, ìˆ˜ì •ì: x2 (30ì )")
    s4_obs_with_x2 = create_obs(s4_hand, ['x2'], full_deck, 50)
    get_q_values(agent, s4_obs_with_x2)
    
    print("  âœ“ ì˜ˆìƒ ê²°ê³¼: Case Bì—ì„œ Stayì˜ Q-valueê°€ ë” ë†’ì•„ì•¼ í•¨")
    print("  âœ“ ì´ëŠ” ì—ì´ì „íŠ¸ê°€ ìˆ˜ì •ì ì¹´ë“œì˜ íš¨ê³¼ë¥¼ ì´í•´í•˜ê³  ìˆìŒì„ ì˜ë¯¸í•¨")
    print()
    
    print("=" * 70)
    print("ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 70)
